

### **Раздел 3 «Алгоритмы и программирование»**

---

#### **Вопросы формирующего оценивания**

1.  **Верно ли утверждение? (Да/Нет):**
    a. Нет (Нейронные сети — это восходящий подход).
    b. Да.
    c. Да.
2.  **Пропуски:** Нейронная сеть состоит из **входного** слоя, **скрытого** слоя(ев) и **выходного** слоя.
3.  **Схема сети:** Входной слой (несколько нейронов) → Полносвязный скрытый слой (несколько нейронов) → Выходной слой (один или несколько нейронов).
4.  **Отличие FCN от CNN:** Полносвязные сети обрабатывают все входы вместе, а сверточные работают с локальными участками данных (фильтрами), что эффективно для изображений.
5.  **Схема нейрона:** Входы → Веса → Сумматор → Функция активации → Выход.
6.  **Модель Мак-Каллока-Питтса vs Перцептрон:** Перцептрон, в отличие от модели Мак-Каллока-Питтса, имел **алгоритм обучения** для автоматической настройки весов.
7.  **Примеры задач:**
    *   **Полносвязная сеть:** Прогнозирование цены квартиры (работа с табличными данными).
    *   **Сверточная сеть:** Распознавание породы собаки на фото (работа с изображениями).
8.  **Нормализация данных:** Нужна, чтобы большие значения не доминировали над малыми, что обеспечивает стабильное и быстрое обучение сети.
9.  **Роль MaxPooling:** Уменьшает размерность изображения (сжатие), сохраняя при этом самые сильные признаки и делая модель устойчивой к небольшим сдвигам объекта.
10. **Улучшение точности:**
    *   Увеличить объем и разнообразие обучающих данных (аугментация).
    *   Усложнить архитектуру сети (добавить слои/нейроны).
    *   Использовать регуляризацию (например, Dropout) для борьбы с переобучением.
11. **Функция Softmax:** Она преобразует выходы сети в распределение вероятностей, где сумма всех значений равна 1, что идеально для задачи многоклассовой классификации.
12. **Аналогия градиентного спуска:** Спуск с горы в густом тумане. Вы не видите всю карту, но на каждом шаге идете в сторону самого крутого уклона вниз, чтобы eventually добраться до самой низкой точки.
13. **Обучение человека vs НС:**
    *   **Сходство:** Оба учатся на примерах (опыт/данные).
    *   **Различие:** Человек учится биологически, а НС — математически, путем подбора весов.
14. **Применения НС:** Распознавание лиц для разблокировки телефона, autopilot в автомобилях, медицинская диагностика по снимкам (КТ, МРТ).

---

#### **Задания для текущего оценивания**

1.  **Соответствие:**
    а | б | в | г
    --|---|---|--
    1 | 2 | 3 | 4
2.  **Работа свёрточного слоя:** Скользит по изображению фильтром (ядром), вычисляя скалярное произведение для выделения локальных признаков (границ, углов).
3.  **Подготовка данных MNIST:** 1. Загрузить изображения и метки. 2. Нормализовать пиксели (к диапазону 0-1). 3. Преобразовать метки в one-hot формат. 4. Разделить на обучающую и тестовую выборки.
4.  **Верные утверждения:**
    `+` Нисходящий подход основан на заранее заданных правилах
    `+` Восходящий подход использует математические модели биологического мозга
5.  **Этапы работы нейрона:**
    1. Получение входных сигналов
    2. Суммирование взвешенных входов
    3. Применение функции активации
    4. Выдача результата
6.  **Функции активации:**
    а | б
    --|--
    2 | 1
7.  **Почему 32 -> 64 фильтра:** На первых слоях сеть ищет простые признаки (линии, точки). На более глубоких слоях она комбинирует их для поиска сложных признаков (формы, объекты), для чего нужно больше фильтров.
8.  **Расчет MaxPooling:** Размер станет **14x14** (28 / 2 = 14).
9.  **Предотвращение переобучения:**
    `+` Добавление слоя Dropout
    `+` Нормализация входных данных
10. **Улучшение CNN для сложных объектов:** Увеличить глубину сети (больше сверточных слоев), использовать больше фильтров, применить аугментацию данных для обучения на более разнообразных примерах.
11. **Размер батча (batch size):**
    *   **Большой:** Быстрое обучение за одну эпоху, stable градиенты. Требует много памяти, может "застрять" в локальном минимуме.
    *   **Маленький:** Медленнее, но лучше обобщает и требует меньше памяти. Градиенты "шумные".
12. **План эксперимента:** 1. Определить сетку гиперпараметров. 2. Написать цикл для перебора их комбинаций. 3. Внутри цикла создавать, обучать модель (на 5 эпохах) и оценивать её на валидационной выборке. 4. Сохранять точность для каждой комбинации. 5. Выбрать комбинацию с наивысшей точностью.

---

#### **Задания для тематического контроля**

**С выбором одного ответа:**
1.  Подход, моделирующий работу мозга: **b) Восходящий**
2.  Функция для вероятностей: **c) Softmax**
3.  Метод минимизации ошибки: **b) Градиентный спуск**

**С выбором нескольких ответов:**
4.  Компоненты нейрона: **a) Входы, b) Веса, c) Функция активации**
5.  **Верные утверждения о подготовке данных:**
    `+` Нормализация переводит значения пикселей в диапазон 0-1
    `+` Данные разделяются на обучающую и тестовую выборки
6.  **Верные утверждения о НС:**
    `+` Нейронные сети способны к обобщению и работе с зашумленными данными
    `+` Нейронные сети требуют большого количества данных для обучения
    `+` Интерпретация работы обученной нейронной сети может быть сложной

**С кратким ответом:**
7.  Ученые: **Мак-Каллок и Питтс**.
8.  Функция MaxPooling2D: **Уменьшает размерность карты признаков, оставляя максимальные значения в окне.**
9.  Процесс свёртки: Фильтр (ядро) "пробегает" по изображению, на каждом шаге вычисляя взвешенную сумму пикселей под ним, создавая карту признаков.
