

### **Раздел 4 «Информационные технологии»**

---

#### **Вопросы формирующего оценивания**

1.  **Верно ли утверждение? (Да/Нет):**
    a) **Да.** k-NN — это классический алгоритм обучения с учителем.
    b) **Нет.** Линейная регрессия предсказывает непрерывные числовые значения, а не классы.
    c) **Да.** k-средних (k-Means) — это основной алгоритм кластеризации.
    d) **Нет.** Деревья решений могут использоваться для многоклассовой классификации и регрессии.
    e) **Да.** LabelEncoder преобразует текстовые метки в числовые.
2.  **Обучение с учителем vs. без учителя:**
    *   **С учителем:** Модель учится на данных, где у каждого примера есть "правильный ответ" (метка). **Задача:** Научиться предсказывать эти ответы для новых данных. *Пример: предсказание цены дома по его характеристикам.*
    *   **Без учителя:** Модель сама ищет скрытые закономерности и структуру в данных, у которых нет меток. **Задача:** Сгруппировать похожие объекты. *Пример: сегментация клиентов на группы по их поведению.*
3.  **Пропуск в определении k-NN:** «Алгоритм k-ближайших соседей присваивает новому объекту ту метку класса, которая **чаще всего встречается среди его k ближайших соседей**».
4.  **Подготовка категориальных данных:**
    *   **Шаги:** 1. Кодировать текстовые значения в числовые с помощью `LabelEncoder` (для порядковых данных) или `OneHotEncoder` (для номинальных).
    *   **Важность:** Большинство алгоритмов ML (включая линейные модели, k-NN, SVM) работают только с числами и математическими операциями. Без кодирования они просто не смогут обработать такие данные.
5.  **k-NN vs. k-средних:**
    *   **k-NN (k-ближайших соседей):** **Классификация/регрессия** (обучение с учителем). Относит новый объект к классу его соседей.
    *   **k-средних (k-Means):** **Кластеризация** (обучение без учителя). Разделяет все данные на заданное число `k` групп (кластеров).
6.  **Проблемы линейной регрессии:**
    *   **Проблема:** Она ищет только линейную зависимость. Если связь между часами учебы и оценкой нелинейная (например, после 4 часов в день эффективность падает), модель будет работать плохо.
    *   **Решение:** Использовать более сложные модели, например, **полиномиальную регрессию** или модели на основе деревьев.
7.  **Пример k-средних:** **Сегментация клиентов.** Интернет-магазин может сгруппировать всех покупателей по их покупательскому поведению (частота покупок, средний чек, категории товаров). Алгоритм k-средних подходит, так как заранее неизвестно, сколько и какие именно группы клиентов существуют. Он сам найдет их.
8.  **Библиотеки ML vs. «с нуля»:**
    *   **Плюсы scikit-learn:** Скорость разработки, проверенные и оптимизированные реализации, единый интерфейс (`fit`, `predict`), отличная документация.
    *   **Минусы scikit-learn:** Меньше гибкости для глубоких кастомизаций алгоритма, "черный ящик", если не понимать, как он работает внутри.
9.  **Визуализация линейной регрессии:** **Диаграмма рассеяния (scatter plot)**. На ней точками наносятся исходные данные (например, по осям "часы учебы" и "оценка"), а затем поверх них проводится прямая линия, построенная моделью. Это наглядно показывает, насколько хорошо модель описывает тренд в данных.
10. **Переобучение:**
    *   **Концепция:** Это когда модель слишком хорошо "запомнила" обучающие данные, включая шум и случайные отклонения, но из-за этого плохо работает на новых, ранее не виденных данных. Она не смогла "обобщить" знания.
    *   **Пример:** Дерево решений, которое строится так глубоко, что каждый лист содержит всего один обучающий пример. Оно идеально классифицирует обучающую выборку, но не сможет правильно классифицировать почти ни один новый объект.
11. **Интерпретация точности 85%:**
    *   **Результат:** Модель правильно определяет класс в 85% случаев на тестовой выборке, которую она никогда не видела. Это хороший, но не идеальный результат.
    *   **Дополнительные метрики:**
        *   **Матрица ошибок:** Показывает, какие именно классы модель путает (например, часто определяет класс "А" как класс "Б").
        *   **Точность (Precision) и Полнота (Recall):** Важны, если классы несбалансированы. Полнота покажет, сколько реальных объектов класса "А" модель вообще нашла.
        *   **F1-Score:** Гармоническое среднее между точностью и полнотой.

---

#### **Задания для текущего оценивания**

1.  **Вид машинного обучения:**
    Вид машинного обучения | Описание ситуации
    ---|---
    **Обучение с учителем** | Модель обучается на наборе данных, где каждый пример имеет правильный ответ.
    **Обучение без учителя** | Алгоритм самостоятельно выявляет группы схожих данных в массиве без предварительной разметки.
    **Обучение с подкреплением** | Робот пробует разные действия и получает вознаграждение за правильные шаги.
2.  **Верные утверждения:**
    `+` Деревья решений могут быть использованы для прогнозирования числовых значений.
    `+` Алгоритм k-ближайних соседей требует разделения данных на обучающую и тестовую выборки.
    `+` Нормализация данных устраняет искажающее влияние признаков с разными масштабами.
3.  **Выбор алгоритма:**
    Алгоритм машинного обучения | Содержание задачи
    ---|---
    **Линейная регрессия** | Прогнозирование стоимости недвижимости на основе исторических данных.
    **K-средних** | Группировка клиентов интернет-магазина по их покупательскому поведению.
    **k-ближайших соседей** | Определение породы животного по его фотографии.
4.  **Процесс подготовки данных:**
    1.  **Нормализация числовых данных** (например, с помощью `StandardScaler`), чтобы все признаки имели схожий масштаб.
    2.  **Кодирование категориальных переменных** (например, с помощью `OneHotEncoder`), чтобы преобразовать их в числовой формат.
    3.  **Разделение** всего набора данных на обучающую и тестовую выборки для объективной оценки модели.
5.  **Интерпретация метрик:**
    Точность (Precision) 92% означает, что из всех объектов, которые модель назвала положительными, 92% действительно таковыми являются. Полнота (Recall) 85% означает, что модель нашла только 85% всех реальных положительных объектов в данных. **Вывод:** модель достаточно точна, но пропускает значительную часть (15%) интересующих нас объектов. Стоит работать над повышением полноты, возможно, в ущерб точности.
6.  **Задача k-NN:**
    *   **Расстояния:** до A(2.23), B(1.41), C(1.0), D(2.23), E(2.0).
    *   **3 ближайших соседа:** C (класс 1), B (класс 0), E (класс 1).
    *   **Результат:** Голосование: 2 за класс 1, 1 за класс 0. **Точка X будет отнесена к классу 1.**
7.  **Уравнение регрессии:**
    *   **Коэффициент 0.5 (наклон):** Показывает, что при увеличении количества часов учебы на 1, прогнозируемая оценка увеличивается на 0.5 балла.
    *   **Коэффициент 2 (свободный член/intercept):** Это базовая оценка, которую модель прогнозирует при нулевом количестве часов учебы.
8.  **Кодирование данных:**
    1.  **Цвет (номинальный признак):** Применить **OneHotEncoder**. 'Красный' -> `[1, 0, 0]`, 'Желтый' -> `[0, 1, 0]`.
    2.  **Вес (числовой признак):** Можно оставить как есть или нормализовать.
    3.  **Название фрукта (целевая переменная):** Применить **LabelEncoder**. 'Яблоко' -> `0`, 'Груша' -> `1`, 'Банан' -> `2`.
9.  **Объяснение кода:**
    *   `clf = DecisionTreeClassifier(...)`: Создается экземпляр модели "Дерево решений" с заданными параметрами (ограничение глубины для предотвращения переобучения).
    *   `clf.fit(X_encoded, y_encoded)`: Модель **обучается** на предоставленных данных `X_encoded` (признаки) и `y_encoded` (правильные ответы).

---

#### **Задания для тематического контроля**

**С выбором одного ответа:**
1.  НЕ относится к обучению с учителем: **d) k-средних**
2.  Метод в k-средних: **d) Метод минимизации суммы квадратов расстояний**
3.  НЕ используется для регрессии: **c) Точность (Accuracy)**

**С выбором нескольких ответов:**
4.  **Верные утверждения о линейной регрессии:**
    `+` Основана на методе наименьших квадратов
    `+` Предсказывает числовые значения
5.  **Шаги подготовки данных:**
    `+` Нормализация числовых данных
    `+` Кодирование категориальных переменных
    `+` Разделение данных на обучающую и тестовую выборку
6.  **Верные утверждения о k-NN:**
    `+` Основан на вычислении расстояний между объектами
    `+` Чувствителен к выбору значения k

**С кратким ответом:**
7.  **Формула энтропии:** `H(S) = - Σ pᵢ * log₂(pᵢ)` , где `pᵢ` — доля объектов класса `i` в наборе `S`.
8.  **Различие обучения:** **С учителем** — у данных есть правильные ответы (метки). **Без учителя** — меток нет, модель ищет структуру сама.
9.  **Нормализация для k-NN:** Чтобы признаки с большим числовым диапазоном (например, зарплата 100000) не доминировали над признаками с малым диапазоном (например, возраст 30) при расчете расстояния.
10. **Построение дерева решений:**
    1.  Выбирается признак, который лучше всего разделяет данные (дает максимальный **информационный выигрыш** или минимальную **энтропию**).
    2.  Данные делятся по этому признаку на узлы.
    3.  Процесс повторяется рекурсивно для каждого нового узла.
    4.  Остановка происходит, когда достигнут **критерий остановки** (например, максимальная глубина дерева, или все объекты в узле принадлежат одному классу).
    
